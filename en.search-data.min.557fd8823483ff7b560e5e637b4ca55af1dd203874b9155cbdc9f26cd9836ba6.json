[{"id":0,"href":"/notes/kl-divergence.html","title":"Kullback-Leibler Divergence (KL Divergence)","section":"Notes","content":"\rKullback-Leibler Divergence (KL Divergence)\r#\rDefinition:\r#\rMeasures the distance between 2 prabability distributions Explanation + Proof:\r#\rBase Video: Intuitively Understanding the KL Divergence - YouTube\rSequence of flips: H -\u0026gt; H -\u0026gt; T \u0026hellip;..\nMultiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:\nfor True coin: P1 raise to something and P2 raise to something else\nFor coin2: Q1 raise to soemthing and Q2 raise to something else\nafter applying log to the RHS: (** \u0026ndash;\u0026gt; Explained at the end)\nAs the number of observations tends towards infinity:\nNh/n ~~ p1\nNt/N ~~ p2\nThis leads us to the final log expression:\nGeneral Fomulae:\r#\r\u0026ldquo;This computes the distance between 2 distributions motivated by looking at how likely the 2nd distribution would be able to generate samples from the first distribution\u0026rdquo;\nCross-entropy Loss is very related to KL Divergence\n** Why take log of probability ?\r#\rFrom the probabilities of ratio, why did we suddenly take log of ratio ??\nThe log of probabilities is closely related entropy. In information theory\r, the entropy of a random variable\ris the average level of \u0026ldquo;information\u0026rdquo;, \u0026ldquo;surprise\u0026rdquo;, or \u0026ldquo;uncertainty\u0026rdquo; inherent to the variable\u0026rsquo;s possible outcomes.\nKL Divergence is also known as relative entropy between 2 distributions.\r#\r"},{"id":1,"href":"/notes.html","title":"Notes","section":"Yogendra Yatnalkar","content":""},{"id":2,"href":"/blogs.html","title":"Blogs","section":"Yogendra Yatnalkar","content":""}]