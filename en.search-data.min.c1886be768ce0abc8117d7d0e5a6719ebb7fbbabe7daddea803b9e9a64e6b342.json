[{"id":0,"href":"/blogs/end-to-end-mlops-on-aws.html","title":"End-to-End MLOps on AWS (3 blogs)","section":"Blogs","content":"\rEnd-to-End MLOps on AWS: Blog Series\r#\rLast Edit Date: December 26, 2023\nI had co-authored 3 blogs out of 7 blogs in total on the topic: \u0026ldquo;End-to-End MLOps on AWS\u0026rdquo;. The blog series is hosted on Quantiphi\u0026rsquo;s Medium Account Other than Quantiphi\u0026rsquo;s Medium account, the blogs are also hosted seperately on github as follows:\nhttps://sagemaker-mlops-samples.github.io/\r#\rAuthors of the below 3 blogs:\nPalash Nimodia\r(Architect — Machine Learning)\nYogendra Yatnalkar\r(Senior Machine Learning Engineer) at Quantiphi\r1. Introducing End-to-End MLOps on AWS: Part1\r#\r2. End-to-End MLOps on AWS: Part2.1 - Computer Vision Simulation with Drift \u0026amp; Retraining\r#\r3. End-to-End MLOps on AWS: Part2.2 - Computer Vision Components and Pipelines Deep Dive\r#\r"},{"id":1,"href":"/blogs/sam-automatic-semantic-segmentation.html","title":"Meta-AI SAM: AutoMatic Semantic Segmentation","section":"Blogs","content":"\rMeta-AI SAM: AutoMatic Semantic Segmentation\r#\rDate: November 26, 2023\r#\rNOTE:\r#\rThe NB was originally developed on Kaggle: https://www.kaggle.com/code/yogendrayatnalkar/finetuning-segment-anything\rRelated Github Repository: https://github.com/yogendra-yatnalkar/SAM-Automatic-Semantic-Segmentation\rTask:\r#\rSemantically segment objects from image AUTOMATICALLY with the help of META AI SAM, without PROMPTS/TRAINING\r#\rSegment all the pepperoni pieces from the pizza topping\r#\rApproach:\r#\r1. Automatic Mask Generation (AMG)\nUtilizing the Segment Anything Model (SAM) from the MetaAI SAM repository, perform instance segmentation on the entire image. This process will identify and isolate individual objects within the image.\n2. Patch-Embedding Extraction \u0026amp; Single Representation per Instance-Segment\nFor each generated instance-segment, extract the corresponding patch embedding. A patch embedding encapsulates a segment\u0026rsquo;s visual features into a concise vector representation.\nTo ensure effective association of each cluster with its corresponding segmentation mask, each instance segmentation mask should have a unique single embedding or single representation. This mapping allows for the accurate assignment of semantic segmentation classes to individual objects.\nTo achieve this, we can extract all the patch embeddings from the encoder features of SAM and average them per segment.\n3. Clustering for Semantic Segmentation\nTreating each segment as a distinct data point, apply clustering algorithms to group similar segments together. Each resulting cluster represents a semantic segmentation class, encompassing objects with shared visual characteristics.\nDetailed Explanation\r#\rPatch Embedding and SAM Encoder\r#\rExtract all patch embeddings from the SAM Encoder.\nThe SAM model takes an input of 1024x1024 pixels.\nThe default patch size is 16x16 pixels.\nTherefore, the input structure is 64x64x16x16, where there are 64x64 patches, each with a size of 16x16 pixels.\nThe SAM Encoder output is 256x64x64.\nAfter shuffling the channels, the output becomes 64x64x256.\nObserve the input-output transformation:\n64x64x16x16 --\u0026gt; 64x64x256 This means that each 16x16 patch is represented by a 256-dimensional embedding vector.\nSegment Embedding\r#\rFor each segment, find the corresponding patches and average their embedding vectors. For example, if there are 30 segments identified using Meta SAM Automatic Mask Generator (AMG), iterate through each segment and: Identify the corresponding patches and obtain their embeddings. For instance, if segment1 corresponds to 3 patches, its patch embedding will be 3x256. Similarly, if segment2 corresponds to 10 patches, its patch embedding will be 10x256. Average all the patch embeddings corresponding to that segment to obtain a single embedding vector. As a result, segment1\u0026rsquo;s embedding vector will be 1x256, segment2\u0026rsquo;s embedding vector will be 1x256, and so on. This process results in a 1x256 embedding vector for each segment. For 30 segments, the representation will be 30x256. Semantic Class Clustering\r#\rCluster the segment embedding vectors using a clustering algorithm, such as DBScan. This algorithm is suitable since the number of segments is unknown. Each cluster formed represents a distinct semantic class.** Setting up of SAM model for Automatic Mask Generation\r#\rimport sys !{sys.executable} -m pip install opencv-python matplotlib !{sys.executable} -m pip install \u0026#39;git+https://github.com/facebookresearch/segment-anything.git\u0026#39; !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth !wget https://i2.wp.com/lifemadesimplebakes.com/wp-content/uploads/2014/09/Classic-Pepperoni-Pizza-1.jpg import cv2 import matplotlib.pyplot as plt import numpy as np import os import torch import torchvision from sklearn.cluster import DBSCAN print(\u0026#34;PyTorch version:\u0026#34;, torch.__version__) print(\u0026#34;Torchvision version:\u0026#34;, torchvision.__version__) print(\u0026#34;CUDA is available:\u0026#34;, torch.cuda.is_available()) import sys from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor /opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version \u0026gt;=1.16.5 and \u0026lt;1.23.0 is required for this version of SciPy (detected version 1.23.5 warnings.warn(f\u0026quot;A NumPy version \u0026gt;={np_minversion} and \u0026lt;{np_maxversion}\u0026quot; PyTorch version: 2.0.0 Torchvision version: 0.15.1 CUDA is available: True # SAM initial model loading if torch.cuda.is_available(): device = \u0026#34;cuda\u0026#34; else: device = \u0026#34;cpu\u0026#34; sam_checkpoint = \u0026#34;/kaggle/working/sam_vit_h_4b8939.pth\u0026#34; model_type = \u0026#34;vit_h\u0026#34; sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) sam.to(device=device) mask_generator = SamAutomaticMaskGenerator(sam) def show_anns(anns): if len(anns) == 0: return sorted_anns = sorted(anns, key=(lambda x: x[\u0026#39;area\u0026#39;]), reverse=True) ax = plt.gca() ax.set_autoscale_on(False) img = np.ones((sorted_anns[0][\u0026#39;segmentation\u0026#39;].shape[0], sorted_anns[0][\u0026#39;segmentation\u0026#39;].shape[1], 4)) img[:,:,3] = 0 for ann in sorted_anns: m = ann[\u0026#39;segmentation\u0026#39;] color_mask = np.concatenate([np.random.random(3), [0.9]]) img[m] = color_mask ax.imshow(img) Sample image\r#\rimg_path = \u0026#34;/kaggle/working/Classic-Pepperoni-Pizza-1.jpg\u0026#34; img = cv2.imread(img_path) # resizing the image to 1024x1024 img = cv2.resize(img, (1024, 1024)) print(\u0026#34;IMG shape: \u0026#34;, img.shape) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.imshow(img_rgb) plt.show() IMG shape: (1024, 1024, 3) Sample image instance segmentation with Automatic-Mask-Generation\r#\rmask_generator = SamAutomaticMaskGenerator( model=sam, points_per_side=51, pred_iou_thresh=0.86, stability_score_thresh=0.92, crop_n_layers=1, crop_n_points_downscale_factor=2, min_mask_region_area=100, # Requires open-cv to run post-processing ) Get the encoder embeddings\r#\rmask_generator.predictor.set_image(img_rgb) enc_emb = mask_generator.predictor.features enc_emb = enc_emb.to(\u0026#34;cpu\u0026#34;).numpy() enc_emb = enc_emb[0].transpose((1,2,0)) print(enc_emb.shape) (64, 64, 256) Generate mask using automatic-mask-generator\r#\rmasks2 = mask_generator.generate(img_rgb) masks2 = sorted(masks2, key=(lambda x: x[\u0026#39;area\u0026#39;]), reverse=True) print(\u0026#34;Number of masks: \u0026#34;, len(masks2)) print(\u0026#34;Shape of individual mask: \u0026#34;,masks2[0][\u0026#39;segmentation\u0026#39;].shape) Number of masks: 264 Shape of individual mask: (1024, 1024) plt.figure(figsize=(20,20)) plt.imshow(img) show_anns(masks2) plt.axis(\u0026#39;off\u0026#39;) plt.show() Get Mask Embedding for each segment:\r#\rMask Embedding = Average of patch embeddings corresponding to individual segment\r#\rNotes:\r#\rAs we can see, there are 264 masks created using Automatic-Mask-Generator The output-shape of encoder is: 256x64x64 \u0026ndash;\u0026gt; which means its a 64x64 image with 256 channels For each mask, get all the corresponding patch-embedding The average of all patch-embeddings for a given mask will be termed as \u0026ldquo;Mask Embedding\u0026rdquo; def get_mask_embedding_using_patch_embeddings(mask, enc_emb, return_all = False): # Converting mask of shape 1024x1024 to shape: 64x64x16x16 # This assumes that patch size is 16x16 becuase what we mainly need is: 64x64 at the start # We are free to change the patch-size accordingly split_mask = np.array(np.split(mask, 64, axis = -1)) split_mask = np.array(np.split(split_mask, 64, axis = -2)) split_mask = split_mask*1 # split_mask is a mask of shape: 64x64x16x16 # split_mask is binary (have value of 0 or 1 not between) # Converting split_mask of shape: 64x64x16x16 to 64x64 # by adding all numbers in every 16x16 grid split_mask = np.sum(split_mask, axis = -1) split_mask = np.sum(split_mask, axis = -1) # Get all patch embeddings from this split_mask of 64x64 # In this split_mask, at all locations where the cell-value is greater than 1, # It means that we need to pick the patch-embeddding at this given index (X,Y) value patch_locations = np.where(split_mask \u0026gt; 1) n_patch_embeddings = enc_emb[patch_locations] mask_embedding = n_patch_embeddings.mean(axis = 0, keepdims = False) if return_all: return mask_embedding, patch_locations, n_patch_embeddings return mask_embedding NOTE:\r#\rAs you can see, post averaging out patch embedding for each segment, we get a mask-embedding of shape: 1x256. Since,we had 264 segments identified, our final vector for 264 segments will be of shape: 264x256. (we will call it Mask Embeddings) These Mask-Embeddings will be passed on to DBScan for clustering, where each individual cluster formed will be considered as a Semantic Class. masks2_embeddings_li = [] for i in range(len(masks2)): nth_mask = masks2[i][\u0026#39;segmentation\u0026#39;] nth_mask_emb = get_mask_embedding_using_patch_embeddings(nth_mask, enc_emb) masks2_embeddings_li.append(nth_mask_emb) masks2_embeddings_arr = np.array(masks2_embeddings_li) print(\u0026#34;Mask Embedding shape: \u0026#34;, masks2_embeddings_arr.shape) Mask Embedding shape: (264, 256) Clustering using DBScan\r#\r# Clustering using DB Scan clustering = DBSCAN( eps=0.06, min_samples=8, metric=\u0026#34;cosine\u0026#34; ).fit(masks2_embeddings_arr) print(\u0026#34;Clustering Labels: \u0026#34;, np.unique(clustering.labels_)) Clustering Labels: [-1 0] Label 0 Semantic Mask\r#\rlabel_0 = np.where(clustering.labels_ == 0) print(\u0026#34;Number of items in cluster 0: \u0026#34;, label_0[0].shape) lbl0_semantic_seg_mask = masks2[-1][\u0026#39;segmentation\u0026#39;]*1 for seg_no in label_0[0]: lbl0_semantic_seg_mask += masks2[seg_no][\u0026#39;segmentation\u0026#39;]*1 # Plottign the label 0 semantic mask plt.imshow(lbl0_semantic_seg_mask) Number of items in cluster 0: (29,) \u0026lt;matplotlib.image.AxesImage at 0x7f1c57f9ee60\u0026gt; Final Output\r#\rbinary_ind = np.where(lbl0_semantic_seg_mask \u0026gt; 0) mask = lbl0_semantic_seg_mask.copy() mask[binary_ind] = 1 img_mask = img_rgb.copy() img_mask[:,:,0] = img_mask[:,:,0]*mask img_mask[:,:,1] = img_mask[:,:,1]*mask img_mask[:,:,2] = img_mask[:,:,2]*mask # Create a figure and three subplots fig, axes = plt.subplots(1, 3, figsize=(12, 4)) # Plot the first image on the first subplot axes[0].imshow(img_rgb, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#39;Input\u0026#39;) axes[0].axis(\u0026#39;off\u0026#39;) # Hide axes # Plot the second image on the second subplot axes[1].imshow(lbl0_semantic_seg_mask, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#39;Label 0 Semantic Map\u0026#39;) axes[1].axis(\u0026#39;off\u0026#39;) # Hide axes # Plot the third image on the third subplot axes[2].imshow(img_mask, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(\u0026#39;Output: Extracted Pepperoni\u0026#39;) axes[2].axis(\u0026#39;off\u0026#39;) # Hide axes # Adjust layout and display the plot plt.tight_layout() plt.show() "},{"id":2,"href":"/blogs/backtracking_aws_lookout_for_vision_service.html","title":"Backtracking AWS Lookout for Vision Service","section":"Blogs","content":"\rBacktracking AWS Lookout For Vision Service\r#\rThe article tries to trace back AWS Lookout for Vision: Edge service model and successfully custom loads the model for inference (Just imagine the reduced inference cost 🔥)\nCo-Author: Palash Nimodia\rDate: June 23, 2022\nMedium Link: https://medium.com/@yogenyat/backtracking-aws-lookout-for-vision-service-136c47c85168\rIntroduction:\r#\rNOTE for the reader: Its fine if you have not used AWS Lookout For Vision service before, but if you are interested in knowing how we can back-track a managed service (if possible 🙈), you are at the right place.\nAmazon Lookout for Vision (LFV) has recently released preview support for anomaly detection at the edge. It is a machine learning (ML) service that spots defects and anomalies in visual representations of manufactured products using computer vision (CV), allowing users to automate quality inspection. ML model is trained to spot anomalies from live production line with as few as 30 images for the process which needs to be visually inspected — with no machine learning experience required. Now, in addition to detecting anomalies in the cloud, Amazon LFV model can be hosted on edge using AWs IoT Greengrass V2 compatible edge devices.\nPrerequisite:\n→ Train a Lookout For Vision Model using AWS Console\n→ Compile and package the model for edge hosting\n→ The below work has been tested on EC2 instance and SageMaker Notebook Instance (EC2 instance as edge) having Nvidia T4 GPU (instance — G4dn.xlarge)\nOnce training and packaging the trained model is complete, our journey starts here.\nWe try to trace-back the service to identify how the LFV service trains its anomaly detection model, what post-processing it performs on the trained model and if there is any minute chance of custom hosting it for inference.\nTHE JOURNEY BEGINS:\r#\rA. Model Packaging:\r#\rA model packaging job packages an Amazon Lookout for Vision model as a model component. While packaging your custom trained model, there is an option for target-device and target-platform. Since we are testing this on EC2 instance, we will choose our target as: target-platform in this case.\nAfter choosing the platform, we will choose compiler-options. Within compiler option, we will have to provide the GPU which we will be using, the tensorrt version and the Cuda library version. For more details, please follow the documentation at the following link: LINK\rOur configuration for G4dn.xlarge Instance:\n{‘gpu-code’: ‘sm_75’, ‘trt-ver’: ‘7.1.3’, ‘cuda-ver’: ‘10.2’}\nSo here comes our first clue, the service is post-processing our model by optimizing it using NVIDIA TensorRT SDK.\nQuestion: We still do not know if the following model is trained using Tensorflow or Pytorch or any other Deep Learning framework… !!\nB. Analyzing Zipped model\r#\rImage Description: Zipped Model Contents\nZipped Model contents\nOnce the model is trained → compiled → TensorRT optimized, it gets saved to AWS S3 in a zipped format. In our case, the zip file and contents within the zip looked something like image above:\nIf we closely observe, there are two things inside it:\nFolder — “mochi” (including the sub-files within this folder) manifest.json file The manifest.json file contains the following contents:\n{\u0026ldquo;model_graph\u0026rdquo;: {\u0026ldquo;model_graph_type\u0026rdquo;: \u0026ldquo;single_stage_model_graph\u0026rdquo;, \u0026ldquo;stages\u0026rdquo;: [{\u0026ldquo;class_normal_ids\u0026rdquo;: [1], \u0026ldquo;seg_normal_ids\u0026rdquo;: [], \u0026ldquo;classification_head_enabled\u0026rdquo;: true, \u0026ldquo;segmentation_head_enabled\u0026rdquo;: false, \u0026ldquo;threshold\u0026rdquo;: 0.7021560668945312, \u0026ldquo;normalize\u0026rdquo;: true, \u0026ldquo;image_range_scale\u0026rdquo;: true, \u0026ldquo;image_width\u0026rdquo;: 1000, \u0026ldquo;image_height\u0026rdquo;: 1000, \u0026ldquo;input_shape\u0026rdquo;: [1, 3, 1000, 1000], \u0026ldquo;type\u0026rdquo;: \u0026ldquo;mochi\u0026rdquo;}], \u0026ldquo;image_level_classes\u0026rdquo;: {\u0026ldquo;names\u0026rdquo;: [\u0026ldquo;anomaly\u0026rdquo;, \u0026ldquo;normal\u0026rdquo;], \u0026ldquo;normal_ids\u0026rdquo;: [1]}, \u0026ldquo;pixel_level_classes\u0026rdquo;: {\u0026ldquo;names\u0026rdquo;: [], \u0026ldquo;normal_ids\u0026rdquo;: []}}, \u0026ldquo;compilable_models\u0026rdquo;: [{\u0026ldquo;filename\u0026rdquo;: \u0026ldquo;mochi.pt\u0026rdquo;, \u0026ldquo;data_input_config\u0026rdquo;: {\u0026ldquo;input\u0026rdquo;: [1, 3, 1000, 1000]}, \u0026ldquo;framework\u0026rdquo;: \u0026ldquo;PYTORCH\u0026rdquo;}], \u0026ldquo;dataset\u0026rdquo;: {\u0026ldquo;image_width\u0026rdquo;: 1000, \u0026ldquo;image_height\u0026rdquo;: 1000}}\nThe Analysis from the folder and JSON file is as follows:\r#\rThe folder name itself is quite unique and hence raises a question: Could “mochi” be some latest/open-source model ? The JSON file contains a key-word named: “mochi.pt” Web-searching on the “mochi” term led to an interesting discovery. It resulted in the following paper: Hard Negative Mixing for Contrastive Learning (Paper Link)\rwhere MoCHI stands for “(M)ixing (o)f (C)ontrastive (H)ard negat(i)ves”. The paper proposes a semi-supervised way of training Deep Learning models using Contrastive Loss where it highlights the importance of “Hard-negatives”. The proposed approach generates synthetic hard negatives on-the-fly for each positive (query). (Note: Please read about contrastive loss and semi-supervised learning for more details …… this was first time for me as-well😢) Illustration of MoCHi\nIt could be assumed that Lookout for Vision has a pre-trained semi-supervised defect detection model. For new model training on customer data, this pre-trained model is further fine-tuned on new data and saved. Hence, this can be also related to the unique feature of the service, which is the need for a very small amount of annotated data as it could be using semi-supervised learning algorithm internally.\nContent within manifest.json file…\nNow, lets have a quick peek at content.json file:\nIt tells us that the trained model is a “Pytorch” model with model name as: mochi.pt. The model is trained and inferred on shape: 1000x1000x3 Even though the service only supports binary classification, the model has 2 output neurons. One output states whether the input image is “normal” or not. Another output states whether the input image is “anomaly” or not. The threshold for detecting output is set at: **0.7021 (at-least to the model trained in our case)\n**- There is key-value pair named: → “normalized”: True. From here, we can assume that the output of the last layer is Normalized using Softmax layer. Now, lets quickly analyze the other sub-files from the “mochi” folder.\nThe libdlr.so file and dlr.h file tells us that the model is compiled using Neo-AI-DLR package (LINK).\r- DLR is a compact, common runtime for deep learning models and decision tree models compiled by AWS SageMaker Neo\r, TVM\r, or Treelite\r. DLR uses the TVM runtime, Treelite runtime, NVIDIA TensorRT™, and can include other hardware-specific runtimes. The libdlr.so file in the model zip specifies the platform details of the compiled model while custom loading in NeoAI-dlr. Hence, it’s required while loading the model on any g4dn.xlarge instance (Nvidia T4 GPU) (Note: Our model was compiled for Nvidia T4 GPU)\nC. Edge Model Custom Loading:\r#\rWe have learnt a lot about this model now. Somehow, lets crack the hidden mystery on how to load it manually without using AWS SDK….\nRequirements:\r#\rInstance with Nvidia T4 GPU (Tested on SageMaker Notebook and EC2 instance of type: g4dn.xlarge) DLR\rinstalled with GPU support. It can be installed by building from source or using **pip with pre-built binaries. (**on development instance, it was installed using: pip install {prebuilt supported binary} ) Edge model zip file locally available and unzipped. Python libraries required: dlr, numpy, cv2, os, torch Image pre-processing before inference:\r#\r(IMPORTANT NOTE: The below listed pre-processing was found using multiple trial and error by comparing custom loaded model inference with the console displayed information)\nRead the image using Opencv (cv2) image library Convert BGR channel image to RGB channel image (Opencv image are read in BGR channel) Resize the image to size: 1000x1000 (3 channel) Normalize the image between scale: 0–1 (divide by 255) Standardize the image with **ImageNet channel wise mean and standard deviation (order: RGB): mean=[0.485, 0.456, 0.406] standard-deviation (std) =[0.229, 0.224, 0.225]** Make the image channel first (Earlier: 1000x100x3, After: 3x1000x1000) Expand image dimension to treat it as batch size 1 for inference. (final dimension: 1x3x1000x1000) Model Loading and Inference:\r#\rWe have finally analyzed multiple things and came to few major conclusions, such as:\nThe training is performed using Pytorch Library The model is TensorRT optimized The model is compiled using NeoAI-DLR package for Nvidia GPU The trained model is a Semi-supervised model (MOCHI) Inference Image Size: 1000x1000x3 Image Pre-processing decoded(mean, standard-deviation, order of image channel) Code to load the model using DLR package and inference. lookout_for_vision_custom_loading.py · GitHub\rimport dlr import numpy as np import cv2 import os # DLR installation # pip install https://neo-ai-dlr-release.s3-us-west-2.amazonaws.com/v1.10.0/gpu/dlr-1.10.0-py3-none-any.whl # Load model. # /path/to/model is a directory containing the compiled model artifacts (.so, .params, .json) model = dlr.DLRModel(\u0026#39;./mochi/\u0026#39;, \u0026#39;gpu\u0026#39;, 0) def process_image(img): # normalizing the image (0-1) and # standardizing with ImageNet Mean and Std-deviation img = img/255 img[:,:,0] = (img[:,:,0] - 0.485)/0.229 img[:,:,1] = (img[:,:,1] - 0.456)/0.224 img[:,:,2] = (img[:,:,2] - 0.406)/0.225 # convert image to channel first from channel last img = img.transpose(2,0,1) # Expanding dimension to treat it as batch-size: 1 img = np.expand_dims(img, axis = 0) return img def predict_on_image(img_path, model): # read image and convert to RGB img = cv2.imread(img_path) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # resize the images if needed # Process image img = process_image(img) # infer and print result y = model.run(img) return round(y[0][0][0], 3), round(y[0][0][1], 3), np.argmax(y) folder_path = \u0026#39;./temp/\u0026#39; for img_name in sorted(os.listdir(folder_path)): if(\u0026#39;.jpg\u0026#39; not in img_name): continue img_path = os.path.join(folder_path, img_name) print(img_name) a,b,c = predict_on_image(img_path, model) print(c,\u0026#39;====\u0026#39;, a,b) We were able to successfully load and infer on our test images using the above attached code. To validate our work, we performed one experiment on it which is as follows:\n— Took one image data-set and converted it to train and test set\n— Train the model on train-set using AWS console and package it for edge such that its zip file is saved in S3. Fetch the zip file and store it in test-environment\n— Infer the model on the cloud using the AWS Console on the test-set. Record its results and model confidence\n— Now, infer using the edge-model on the testing instance using the above code on the test-set images. Record the results and confidence score\nResult:\r#\rWe observed that, when the test-set was inferred on the edge using custom loading of the model, the inference results and the confidence scores were exactly identical as compared with the AWS LFV Console inference.\nWith this in hand, we can easily see how much cost we could save in future, as we will only have to bear the training cost and avoid all the AWS API/SDK inference cost. Lets say, we are on a NVIDIA Jetson device and we have trained a AWS LFV model, we will be able to directly infer on new images/video-frames using custom loading.\nIf you have actually read this much and liked it, please do not forget to give a clap and subscribe for future articles…..\nTHE END…..\nTags AWS, Computer Vision, Pytorch, Deep Learning, Cloud Computing "},{"id":3,"href":"/blogs/finding-nth-aggregate-from-every-group-aws-athena.html","title":"Finding the n’th Aggregate Value from Every Group in AWS Athena/Presto","section":"Blogs","content":"\rFinding the n’th Aggregate Value from Every Group in AWS Athena/Presto\r#\rCo-Author: Palash Nimodia\rDate: June 2, 2022\nMedium Link: https://medium.com/selectfrom/finding-the-nth-aggregate-value-from-every-group-in-aws-athena-presto-1da505310901\rPrerequisite:\r#\rBefore going ahead, a quick read for those who don’t know what AWS Athena and Presto are.\nAWS Athena: Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is easy to use, fast, scalable and serverless, so there is no infrastructure to manage, and we pay only for the queries that we run. We also don’t have to pay for failed queries. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.\nPresto: Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. AWS Athena uses Presto internally as its query engine.\nIntroduction:\r#\rContinuing on with our main focus, today we will discuss finding the nth aggregate value from every group in AWS Athena/Presto).\nLet’s take an example: finding some aggregate value from every group in SQL is possible and its solutions are quite easily available. Across the few solutions that I went through, two things were common:\nIt always referred to the 2nd highest/lowest. Most of the solutions used SQL variables and inner join. Now imagine a scenario where you are required to find the 5th…. 6th…. nth largest/smallest/aggregate value from every group where data size is HUGE (in TB’s) and you are restricted from using SQL variables.\nIn such scenarios, due to the sheer scale, inner-join will be a very heavy operation and hence its best to avoid it. Another restriction being the use of variables in AWS Athena. Since AWS Athena is used for data analytics and not transactional databases, the use of variables is not supported as of now.\nSo after the above discussion, is there any other way ??? …… Yes, there is. Let’s have a quick look at the dummy data-set first and then move onto our solution.\nThe dummy data-set has 1,000 rows and 4 columns. The “id” column has 10 unique values of type varchar. The “date” column is of type date-type. The “class-type” column is of data-type varchar containing values namely: “extra”, “vip”, “normal”. The “class-marks” column contains integers ranging between 0 and 100.\nThis is how the dummy data-set looks\nData Download: The dummy data-set is very small in size and it is uploaded on GitHub — augmented-data.csv\rLet’s define a question now:\nQuestion 1:\r#\rFor every ID, find the 5th highest scorer from every unique “class-type”.\nPre-work: The above data-set is uploaded on AWS S3 and crawled using AWS Glue. This crawled table is now easily accessible using AWS Athena.\nSolution:\r#\rLet’s divide the solution into 3 parts, hence containing 3 nested queries. (The entire query is attached at part 3)\nPart 1:\r#\r— — — — — —\nAs we are required to compute the highest scorer, we will need to sort the table based on marks in descending order. The query to perform that will be as follows:\n— — — — — —\nQuery1:\r#\rWITH \u0026#34;sorted_marks_table\u0026#34; AS( SELECT * FROM \u0026#34;augmented_data\u0026#34; ORDER BY \u0026#34;augmented_data\u0026#34;.\u0026#34;class-marks\u0026#34; DESC ) SELECT * FROM \u0026#34;sorted_marks_table\u0026#34;; Query1 Output:\r#\raugmented_data table in sorted order\nPart2:\r#\r— — — — — —\nIn the next part, let’s work on grouping. We need to compute the highest scorer for every ID, hence we will need to group by on the ID column. In this sub-query, we will create a mapping (you can imagine a hash-map/dictionary) between “class-type” and “class-marks” column and group by on “id” column. It will be such that for every unique id the map key will be unique class type and value will be a list consisting of all the marks associated with that id and class name.\nPlease observe the output image, since we had already sorted the table based on marks in the earlier sub-query, the value list is also sorted in the highest to lowest marks(descending).\n— — — — — —\nQuery2 (In combination with Query1):\r#\rWITH \u0026#34;sorted_marks_table\u0026#34; AS( SELECT * FROM \u0026#34;augmented_data\u0026#34; ORDER BY \u0026#34;augmented_data\u0026#34;.\u0026#34;class-marks\u0026#34; DESC ), \u0026#34;mapping_table\u0026#34; AS ( SELECT id, multimap_agg(\u0026#34;class-type\u0026#34;, \u0026#34;class-marks\u0026#34;) AS \u0026#34;mapping_col\u0026#34; FROM \u0026#34;sorted_marks_table\u0026#34; GROUP BY \u0026#34;id\u0026#34; ) SELECT * FROM \u0026#34;mapping_table\u0026#34;; Query2 Output:\r#\rTable grouped on “id” column such that all the marks are associated with the unique class-types.\nPart3 (Final):\r#\r— — — — — —\nNow, in the final part we just need to display the required results from our map. As we have all the marks in sorted order for every class type and we need the 5th highest scorer, we will select the 5th place value from every key of the map.\nOne caveat of this solution is that we need to know the unique keys beforehand and need to explicitly specify the key-name. But there is a reason behind this, unlike some programming languages like Python or Java, we are using a form of SQL and we need to know the schema of the Database before-hand.\n— — — — — —\nQuery3:\r#\rWITH \u0026#34;sorted_marks_table\u0026#34; AS( SELECT * FROM \u0026#34;augmented_data\u0026#34; ORDER BY \u0026#34;augmented_data\u0026#34;.\u0026#34;class-marks\u0026#34; DESC ), \u0026#34;mapping_table\u0026#34; AS ( SELECT id, multimap_agg(\u0026#34;class-type\u0026#34;, \u0026#34;class-marks\u0026#34;) AS \u0026#34;mapping_col\u0026#34; FROM \u0026#34;sorted_marks_table\u0026#34; GROUP BY \u0026#34;id\u0026#34; ) SELECT \u0026#34;id\u0026#34;, mapping_col [ \u0026#39;normal\u0026#39; ] [ 5 ] as \u0026#34;normal_max_5th\u0026#34;, mapping_col [ \u0026#39;vip\u0026#39; ] [ 5 ] as \u0026#34;vip_max_5th\u0026#34;, mapping_col [ \u0026#39;extra\u0026#39; ] [ 5 ] as \u0026#34;extra_max_5th\u0026#34; FROM \u0026#34;mapping_table\u0026#34;; Query3 Output:\r#\rFinal output containing the 5th highest marks from every‘ class-type’ for every ‘id’\nThat’s it! We are done and dusted. We have found the 5th largest scorer for every id for every “class-type”.\n— — — — — — — — — — — — — — — — — — — — — — — — — —\nAre we not done? Why is this blog not ending here?\nLet’s try computing another very similar question.\nQuestion2:\r#\rFor every ID, find the 3rd lowest scorer from every unique “class-type”\r#\rSolution:\r#\rWITH \u0026#34;sorted_marks_table\u0026#34; AS( SELECT * FROM \u0026#34;augmented_data\u0026#34; ORDER BY \u0026#34;augmented_data\u0026#34;.\u0026#34;class-marks\u0026#34; ), \u0026#34;mapping_table\u0026#34; AS ( SELECT id, multimap_agg(\u0026#34;class-type\u0026#34;, \u0026#34;class-marks\u0026#34;) AS \u0026#34;mapping_col\u0026#34; FROM \u0026#34;sorted_marks_table\u0026#34; GROUP BY \u0026#34;id\u0026#34; ) SELECT \u0026#34;id\u0026#34;, mapping_col [ \u0026#39;normal\u0026#39; ] [ 3 ] as \u0026#34;normal_max_3rd\u0026#34;, mapping_col [ \u0026#39;vip\u0026#39; ] [ 3 ] as \u0026#34;vip_max_3rd\u0026#34;, mapping_col [ \u0026#39;extra\u0026#39; ] [ 3 ] as \u0026#34;extra_max_3rd\u0026#34; FROM \u0026#34;mapping_table\u0026#34;; Question2 Output:\r#\rFinal output containing the 3rd lowest marks from every‘ class-type’ for every ‘id’\nSo what did we change:\nWe sorted the table in ascending order in the first part of the query and selected the 3rd element this time.\nThat’s it! GG! 👏\nI hope you found this article instructional and informative. If you have any feedback or queries, please let me know in the comments below.\nTags AWS, SQL, Presto, Data Science, Data "},{"id":4,"href":"/notes/general/benchmarking-with-torchserve.html","title":"Benchmarking Inference with Torchserve","section":"General","content":"\rBenchmarking Inference with Torchserve\r#\rLast Edited 24/12/2023 Pytorch default - g4dn.xlarge\r#\rNotes:\r#\rInstance Type: ml.g4dn.xlarge\nGPU: Nvidia T4 vCPU no: 4 CPU memory: 16 GB GPU memory: 16 GB Max RPS achieved: 32\nWith various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 32, the max RPS possible was only 32. Locust Configuration: Max Users: 200, Spawn Rate: 10 Max Response time at 95th percentile: ~5-6 sec\nConfiguration:\r#\renable_envvars_config=true\rload_models=all\rmodel_store=./model_store\rmodels={\\\r\u0026#34;vit_l_16\u0026#34;: {\\\r\u0026#34;1.0\u0026#34;: {\\\r\u0026#34;defaultVersion\u0026#34;: true,\\\r\u0026#34;marName\u0026#34;: \u0026#34;vit_l_16.mar\u0026#34;,\\\r\u0026#34;minWorkers\u0026#34;: 4,\\\r\u0026#34;maxWorkers\u0026#34;: 4,\\\r\u0026#34;batchSize\u0026#34;: 16,\\\r\u0026#34;maxBatchDelay\u0026#34;: 50\\\r}\\\r}\\\r} Pytorch default - g4dn.2xlarge\r#\rNotes:\r#\rInstance Type: ml.g4dn.2xlarge\nGPU: Nvidia T4 vCPU no: 8 CPU memory: 32 GB GPU memory: 16 GB Max RPS achieved: 32\nWith various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 64, the max RPS possible was only around 32. Locust Configuration: Max Users: 200, Spawn Rate: 10 Max Response time at 95th percentile: ~5-6 sec\nIt can be noted that the GPU utilization is at 100% but the gpu memory is underutilized and the vCPU\u0026rsquo;s are also unutilized Never the less, with any change in configuration in number-of-model-workers or batch-size or delay, the results and utilization numbers does not change. Configuration:\r#\renable_envvars_config=true\rload_models=all\rmodel_store=./model_store\rmodels={\\\r\u0026#34;vit_l_16\u0026#34;: {\\\r\u0026#34;1.0\u0026#34;: {\\\r\u0026#34;defaultVersion\u0026#34;: true,\\\r\u0026#34;marName\u0026#34;: \u0026#34;vit_l_16.mar\u0026#34;,\\\r\u0026#34;minWorkers\u0026#34;: 1,\\\r\u0026#34;maxWorkers\u0026#34;: 1,\\\r\u0026#34;batchSize\u0026#34;: 64,\\\r\u0026#34;maxBatchDelay\u0026#34;: 200,\\\r\u0026#34;responseTimeout\u0026#34;: 240\\\r}\\\r}\\\r} "},{"id":5,"href":"/notes/nlp/bert.html","title":"BERT","section":"NLP","content":" BERT (Bidirectional Encoder Representation From Transformer)\r#\rSource:\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - YouTube\rOriginal Paper: https://arxiv.org/pdf/1810.04805v2.pdf\rSource:\nBERT Neural Network - EXPLAINED! - YouTube\rBefore BERT:\r#\rLSTM\u0026rsquo;s were used.\nProblems:\nSlow as each word is processed at a time (sequentially) Not truly bi-directional (left to right and right to left at a time in bidirectional LSTM) Bert Architecture: Multiple encoders stacked on each-other\nPretraining and Finetuning\nPretraining Task is used to learn the language and its context. It is done using two tasks:\nMask Language Model (MLM):\nSentence sentence {Fill_in_the_Blanks} remaining sentence\nHelps bert understand the bidirectional meaning of a sentence\nNext Sentence Prediction (NSP):\nPredict whether the a given sentence is the next sentence of the current sentence. Like a binary classification task.\nIt helps bert in understanding context across different sentences.\nUsually, the MLM task and NSP task are performed simultaneously.\nFinetuning:\nFinetune on task specific data.\nFast and compute efficient\nOnly replace last few layers of the original architecture\n"},{"id":6,"href":"/notes/general/general-general.html","title":"General Notes","section":"General","content":"\rLinux:\r#\rWget vs Curl\r#\rcurl and wget both support http and various types of FTP protocols. curl is library but wget is a CLI tool. curl is used for both way data transfer (src to detination and vice-versa) But, wget can be used only for single way data transfer, example: downloading something form a web-server. "},{"id":7,"href":"/notes/general/kl-divergence.html","title":"Kullback-Leibler Divergence (KL Divergence)","section":"General","content":"\rKullback-Leibler Divergence (KL Divergence)\r#\rLast Edited 25/06/2023 Definition:\r#\rMeasures the distance between 2 prabability distributions Explanation + Proof:\r#\rBase Video: Intuitively Understanding the KL Divergence - YouTube\rSequence of flips: H -\u0026gt; H -\u0026gt; T \u0026hellip;..\nMultiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:\nfor True coin: P1 raise to something and P2 raise to something else\nFor coin2: Q1 raise to soemthing and Q2 raise to something else\nafter applying log to the RHS: (** \u0026ndash;\u0026gt; Explained at the end)\nAs the number of observations tends towards infinity:\nNh/n ~~ p1\nNt/N ~~ p2\nThis leads us to the final log expression:\nGeneral Formulae:\r#\r\u0026ldquo;This computes the distance between 2 distributions motivated by looking at how likely the 2nd distribution would be able to generate samples from the first distribution\u0026rdquo;\nCross-entropy Loss is very related to KL Divergence\nImportant Notes:\r#\rKL Divergence is un-symmetric i.e the divergence depends on the distribution placed on the denominator.\nIn other words: Divergence of distribution1 wrt distribution2 is not same as divergence of distribution2 wrt distribution1.\n** Why take log of probability ?\r#\rFrom the probabilities of ratio, why did we suddenly take log of ratio ??\nThe log of probabilities is closely related entropy. In information theory\r, the entropy of a random variable\ris the average level of \u0026ldquo;information\u0026rdquo;, \u0026ldquo;surprise\u0026rdquo;, or \u0026ldquo;uncertainty\u0026rdquo; inherent to the variable\u0026rsquo;s possible outcomes.\nKL Divergence is also known as relative entropy between 2 distributions.\r#\rFor good reference to entropy, watch statquest video:\nEntropy (for data science) Clearly Explained!!! - YouTube\r"},{"id":8,"href":"/notes/general/model-serving.html","title":"Model Serving","section":"General","content":"\rNotes on Model Serving\r#\rExample: Torchserve, Tf-Serving, Triton, Flask, etc\nTorchServe: - to check model status, I am using port 8081 - for inference, I am using port 8080 - if not using ts-config while deploying a model, it generates error - Question: - when to use 8080 vs 8081 \u0026ndash;\u0026gt; Inference api is bind to 8080, management api is bind to 8081 - how to load test ? \u0026ndash;\u0026gt; locusts (fairly easy to use) -\nserve/examples/image_classifier/mnist/\ntorch-model-archiver \u0026ndash;model-name mnist \u0026ndash;version 1.0 \u0026ndash;model-file mnist.py \u0026ndash;serialized-file mnist_cnn.pt \u0026ndash;handler mnist_handler.py\nmkdir model_store mv mnist.mar model_store/ torchserve \u0026ndash;start \u0026ndash;model-store model_store \u0026ndash;models mnist=mnist.mar \u0026ndash;workers 4 \u0026ndash;ts-config config.properties curl http://127.0.0.1:8080/predictions/mnist -T test_data/0.png\n"},{"id":9,"href":"/notes/nlp/nlp_general.html","title":"NLP-General","section":"NLP","content":"\rNLP General:\r#\rI will keep on appending stuff which I read about NLP as and when I get time in this place This is mainly intended for two things: Quick glance on what I had read in past for a given topic If needed to deep-dive, just look at the sources I used while reading it for the first time Hackers Guide to Language Model:\r#\rSource: A Hackers' Guide to Language Models - YouTube\rNotebook: GitHub - fastai/lm-hackers: Hackers' Guide to Language Models\rDate: 02/11/2023\nOn a high note, what is a language mode:\nPredicts the next word\nor predicts the missing word\nByte-Pair-Encoding tokenizer for OpenAI models: GitHub - openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI's models.\r\u0026ldquo;NN has got the ability to create rich hierarchy of abstractions and representations on the base training data which is clearly a form of knowledge compression.\u0026rdquo;\nLM\u0026rsquo;s (mainly LLM\u0026rsquo;s) are trained in 3 parts:\nPretrained: Unsupervised training on large corpus of data and building a generalized model. The tasks can be:\nNext word prediction\nMask word and predict the masked word\nLLM Fine-Tuning: Again unsupervised training like next word prediction, but on small amount of task-specific data\nNote: In Computer vision mainly, when we say fine-tuning, we usually train only a small part of model (usually last few layers) again. But in the case of LLM\u0026rsquo;s, we retrain the entire Model.\nWithin this second step, the paradigm has recently shifted to Instruction Finetuning. In this, the input text is of the form: Instruction \u0026ndash;\u0026gt; Context \u0026ndash;\u0026gt; Question. For further knowledge, a good source is: Instruction Fine-tuning of Large Language Models | Niklas Heidloff\rLlama2-chat models are instruction fine-tuned where the instruction of llama2-chat is quite long for example:\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\r\u0026lt;\u0026lt;CONTEXT\u0026gt;\u0026gt;\rQuestion: \u0026lt;\u0026lt;Question\u0026gt;\u0026gt; Classifier fine-tuning:\nReinforcement Learning with Human Feedback:\nGood source to learn is from Yannick\u0026rsquo;s video: Learning to summarize from human feedback (Paper Explained) - YouTube\r(This is the original paper released in 2020 by OpenAI)\nFrom my current understanding, there will be a separate scoring model\nThe LLM would generate some output for the input and scoring model will generate the score for the output.\nThe scoring model will be trained on small supervised data where the scores are generated by humans.\nAs of September 2023, GPT4 is the best SOTA LLM. Listing down few things which GPT4 cannot do:\nHallucinations\nIt doesn\u0026rsquo;t know about itself. (Why not?) \u0026ndash;\u0026gt; Because it was not included in its training and due to RLHF, it is just hallucinating\nIt doesn\u0026rsquo;t know about URLs.\nKnowledge cutoff\nOpenAI Cost:\nSmall Note on GPU Performance per price:\nHigher priced GPU is not always as good as its price because we need GPU having ultra-fast memory transfer speed as compared to ultra-fast compute operations. How to make inference faster in Casual Language Model:\r#\rTo increase performance of LLM prediction, we should try to reduce output tokens as compared to input tokens. It will have HUGE Impact.\nKV Cache\nContinuous Batching (different than dynamic batching)\nPaged Attention\nFor Nvidia GPUs, use dtype: bfloat16 instead of fp16. The \u0026ldquo;blfoat16\u0026rdquo; dtype is Nvidia Sepecfic which uses more memory but prvides faster compute.\nFlash Attention (v2 at the time of writing) \u0026ndash;\u0026gt; Need to confirm if the principles of Flash-Attention are also application to non-Nvidia GPU\u0026rsquo;s. Eg: Inferentia/TPU\n"},{"id":10,"href":"/notes/cv/sam-segment-anything.html","title":"SAM-Segment-Anything","section":"CV","content":"\rSegment Anything (SAM)\r#\rLast Edited 16/07/2023 Source: - Original Paper (notes till page no 7) - Youtube: https://www.youtube.com/watch?v=eYhvJR4zFUM Introduction:\r#\rFoundation model for segmentation.\nImportant thing to note here, is that SAM is not just for semantic segmentation but can also be used for instance or panoptic or salient segmentation as well. We just need to engineer it accordingly.\nTrained ON: 11 million images and 1 billion corresponding masks\nEarlier foundation model: Clip and Align \u0026raquo; Now getting used in other downstream tasks like image generation (DALL-E).\nTo build foundation mode, 3 things are necessary:\nTask\nModel\nDataset and Data-Engine\nTask\r#\rTask: Prompt-able segmentation task \u0026raquo; prompt can be text or mask or BB or points \u0026raquo; even if prompt is not accurate or it is confusing, the output should be good.\nZero-shot transfer: Example- if you have a OD model to detect cats, SAM can be used to segment each cat (instance segmentation) in your image. If you want semantic segmentation, convert that cat instances to single class.\nSAM vs Other multi-segmentation models:\nOther models are not general. They do a set of predefined tasks they are trained on, they can be many but fixed.\nIn case of SAM, it can be engineered to perform any task with itself alone or with other systems (example: OD model or older segmentation model). The prompt makes it key here and forms a general foundation model.\nModel:\r#\rModel: Heavy and powerful image encoder, light prompt encoder, light++ prompt and image decoder.\nImport thing to note: for faster inference, the encoder can be hosted online on server, but prompt encoder and decoder can be hosted on edge (eg. browser). Hence reducing the network latency. ++ We need to encode only once, then we can prompt it as many times as needed.\nSAM predicts multiple masks for single image (to battle confusion)\nImage Encoder: Image encoder is pretrained ViT from MAE (Masked Auto-Encoders are Scalable Vision Learners). In MAE, random patches from ViT are masked and an autoencoder is trained to predict the masked patch.\nSAM uses this MAE encoder as its base encoder\nPrompt Encoder:\nText: Encoded using CLIP\nBounding box coordinates or point coordinates: Positional Encoder with summed learned embedding\nMask: Embedding from CNN\nMask Decoder:\nInspired from original Transformer decoder. (modified)\nOutput of decoder goes to dynamic prediction head\nUses self-attention and cross-attention between prompt and image-embedding in bi-directional way (i.e prompt-to-image embedding and vice-versa)\nLater, up-sample and pass it to a linear classifier\nAmbiguity and Backprop: For each input, 3 masks are predicted and during backprop for loss computation, the mask with least amount of loss is counted. Loss is computed using IoU.\nEfficiency: Post encoder embedding, the prompt encoder and decoder can work on CPU (web-browser) with 50 ms latency.\nLoss metrics used: Focal loss and Dice Loss\nFocal Loss: Focal Loss is am improved version of Cross-Entropy Loss that tries to handle the class imbalance problem by down-weighting easy negative class and focusing training on hard positive classes. In paper, Focal Loss is mathematically defined as: Reference: Understanding Cross-Entropy Loss and Focal Loss | Multiplying Matrices for a living (theguywithblacktie.github.io)\r(Do read it, very good explanation) Note: Focal loss was initially used for object detection (invented at FAIR)\nDice Loss: Dice loss is 1 - Dice Coefficient. You can directly imagine Dice Coefficient as IoU.\nDataset\r#\rDataset and Data-Engine: 11 mil images, 1 billion masks. Tag with the help of model, retrain and retag \u0026ndash;\u0026gt; Cycle continues\u0026hellip;\nResponsible AI: Images are taken from all over the globe across all human species. "},{"id":11,"href":"/notes/cv/segformer.html","title":"SegFormer: Segmentation using Transformer","section":"CV","content":"\rSegFormer: Segmentation using Transformer\r#\rLast Edited 16/07/2023 Source: SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers - YouTube\rInput patch size: 4x4\nIn ViT, it was 16x16, but with smaller path size, the authors said, smaller batch size is better (and required) for dense prediction.\nNote: With reducing the patch size, the computation increases.\nAfter each transformer block (encoder in this case), there is a feed-forward block which is mainly used to lower the dimension like older UNet \u0026ndash;\u0026gt; i.e in HxWxC \u0026hellip;. as we go ahead in the encoder block, the height and width will decrease but the number of channels will keep on increasing.\nIn normal ViT as well, there are multiple transformer blocks, output dimension of each block of encoder is same its input.\nThese MLP blocks used in the encoder (Not transformer encoder), not just downsample the input (and increase channels) but also dynamically adjusts the patch embedding vector passed on to the next transformer block.\nEfficient self-attention reduces the sequence to lower computation cost (NOTE: Later need to read about efficient self attention)\nThe segformer decoder consists of Dense NN (MLP) which has upsamping blocks in between.\nImportant: One big achievement of segformer is, it not just has high mIoU, its Params size is also very small\u0026hellip;.. hence inference speed is also high.\n"},{"id":12,"href":"/notes/cv/self-supervised-learning.html","title":"Self Supervised Learning","section":"CV","content":"\rA Cookbook of Self-Supervised Learning:\r#\rInitial Notes from: https://arxiv.org/abs/2304.12210 Intro:\r#\rNLP advanced due to SSL \u0026ndash;\u0026gt; No need of labelled data to train supervised model\nSSL -\u0026gt; Define a pretext task \u0026ndash;\u0026gt; Un-labelled data \u0026ndash;\u0026gt; intelligent representation\nNLP: Word2Vec is SSL \u0026ndash; In a sentence, mask a word and predict the surrounding words (It learns context)\nCV: 2 current popular ways:\nmask a patch and prediction of masked path\naugmented version of the same sample \u0026ndash;\u0026gt; train model such that embeddings from these 2 images are close as compared to any other image.\nWhy SSL is hard and need of cookbook\nComputational Cost\nNo detailed papers and its proper implementation with parameters\nunified vocab\nOrigin of SSL:\r#\rDiscussion about several pre-text tasks which were used few years ago in the field of SSL:\nInformation restoration:\nRemove something from image and restore it or convert to grayscale and train a ML model to predict the colors. This helps in learning object semantics and boundaries.\nNewer Method: Masked-AutoEncoding - Transformer based where patches are masked\nVideo Temporal Relationship:\nModel training using triplet loss for similarity of two representations of same object in 2 different frames.\nRemove audio track and predict it based on the video input\nPrediction of depth mapping between un-labelled image pairs.\nLearning spatial context:\nRandom rotation \u0026ndash;\u0026gt; predict the amount of rotation\nJigsaw: convert image to blocks and create pairs \u0026ndash;\u0026gt; predict the relative position of each pair.\n:\n"},{"id":13,"href":"/notes/nlp/transformers_at_training_vs_inference.html","title":"Transformers at Training vs Inference","section":"NLP","content":"\rTransformers at Training vs Inference\r#\r"},{"id":14,"href":"/notes/general/unanswered-questions.html","title":"Un-Answered Questions","section":"General","content":"\rUn-Answered Questions:\r#\rDifference between Float16 vs Bfloat16 vs Tensor-Float16 ?\nVector Databases: HNSW vs IVF ?\nDifference between vector DB\u0026rsquo;s and FAISS library (by Meta) ?\nFrom my current knowledge both are same, but then why is everyone behind vector DB\u0026rsquo;s instead of using FAISS directly ? Null Hypothesis test \u0026raquo; p-values \u0026raquo; calculated using t-test or z-test\n"},{"id":15,"href":"/notes/general/vector-store-and-search.html","title":"Vector Search and Stores","section":"General","content":"Note: Just putting down few notes from AWS partner-cast session\nVector Search and Vector Stores\r#\rHow to measure similarity in embeddings?\r#\rCosine Similarity: Gives the angle between the 2 embeddings. Higher the angle, bigger is the difference between 2 embeddings.\nDot-Product: Same as cosine similarity but gives us the magnitude between 2 vectors instead of direction/angle.\nReal-life Use cases:\r#\rSemantic search\nRecommendation System\nAnomaly detection and pattern recognition\nGenAI: RAG (Retrieval Augmented Generation)\nRAG Implementation in AWS:\r#\rVector embedding is used to find top 3 most similar chunks\nIn the prompts, the context provided needs to be given in natural language i.e english and not in embeddings format\nRead about HNSW: Hierarchical Navigable Small Worlds (HNSW) | Pinecone\rand Nearest Neighbor Indexes: What Are ivfflat Indexes in pgvector and How Do They Work\rVector search is read-only\nRead about: Knn search vs ANN search\nRead about: Postgres integration with aurora and rds for vector search\n"},{"id":16,"href":"/notes/general/api-performance-improvement.html","title":"Web-API performance improvement","section":"General","content":"\rAPI Performance Improvement\r#\rBased on: Top 7 Ways to 10x Your API Performance - YouTube\rOptimization should not be the first step of development\n1. Caching:\r#\rIf same request is repeated multiple times \u0026ndash;\u0026gt; cache hence no need to recompute or hit the DB again.\nFor DB, its: MemCacheD or Redis\n2. Connection Pooling:\r#\rHaving continues connections with DB can slow down server as each connection requires a lot of handshake protocol. Hence, it s a good practice to already have a set of connections ready with each set of API. This is difficult in serverless applications like Lambda and can cause big problems:\nSolution (at-least on AWs): RDS Proxy:- It sits between DB and applications (including AWS Lambda) to efficiently manage the DB connections\n3. N+1 query problem:\r#\rIdeally, we should fetch data in a single request to Db instead of asking or querying it N times. Conclusion being, we should try to club requests to query our DB. 4. Pagination:\r#\rIf data to be fetched or requested from DB or server is huge, it will slow the response time \u0026ndash;\u0026gt; Hence we should paginate our response into multiple pages to reduce data transfer in single go.\n5. Json Serialization:\r#\rSerialization takes time \u0026hellip;hence consider ways to reduce that time\nExample: Can think of using gRPC or some json serialization library which is very fast.\n6. Compress API response payloads to reduce network latency.\r#\rGitHub - google/brotli: Brotli compression format\rVarious CDN also perform these tasks example: Cloudfare\n7. Async Logging:\r#\rLogging is important but writing logs during stream processing applications can cause bottleneck\nHence, in such scenarios it is better to log logs via async operations.\nBut, there is a small chance that the some logs can be missed in this case.\n"},{"id":17,"href":"/notes/general.html","title":"General","section":"Notes","content":""},{"id":18,"href":"/blogs.html","title":"Blogs","section":"","content":"\rList of Blogs:\r#\rEnd-to-End MLOps on AWS (3 blogs)\r#\rDate: December 26, 2023\nGithub Hosting: https://sagemaker-mlops-samples.github.io/\rMedium Link: Introducing End-to-End MLOps on AWS: Part1\r===|||=== End-to-End MLOps on AWS: Part2.1 - Computer Vision Simulation with Drift \u0026amp; Retraining\r===|||=== End-to-End MLOps on AWS: Part2.2 - Computer Vision Components and Pipelines Deep Dive\rMeta-AI SAM: AutoMatic Semantic Segmentation\r#\rDate: November 26, 2023\nKaggle Link: https://www.kaggle.com/code/yogendrayatnalkar/finetuning-segment-anything\rGithub Link: https://github.com/yogendra-yatnalkar/SAM-Automatic-Semantic-Segmentation\rBacktracking AWS Lookout for Vision Service\r#\rDate: June 23, 2022\nMedium Link: https://medium.com/@yogenyat/backtracking-aws-lookout-for-vision-service-136c47c85168\rFinding the n’th Aggregate Value from Every Group in AWS Athena/Presto\r#\rDate: June 2, 2022\nMedium Link: https://medium.com/selectfrom/finding-the-nth-aggregate-value-from-every-group-in-aws-athena-presto-1da505310901\r"},{"id":19,"href":"/notes/cv.html","title":"CV","section":"Notes","content":""},{"id":20,"href":"/notes/nlp.html","title":"NLP","section":"Notes","content":""},{"id":21,"href":"/notes.html","title":"Notes","section":"","content":""},{"id":22,"href":"/project-experience.html","title":"Project Experience","section":"","content":"\rMy projects and industry experience:\r#\r{WORK IN PROGRESS: Sorry, I am yet to write things in this section. I have just created the template as of now….. }\nAdd this later in the main page: (markdown text)\n"},{"id":23,"href":"/resume/","title":"Resume","section":"","content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r"}]