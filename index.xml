<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yogendra Y.</title>
    <link>https://yogendra-yatnalkar.github.io/</link>
    <description>Recent content on Yogendra Y.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://yogendra-yatnalkar.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Promtless Task-Specific Finetuning of MetaAI Segment-Anything</title>
      <link>https://yogendra-yatnalkar.github.io/blogs/promtless-taskspecific-finetuning-segment-anything.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/blogs/promtless-taskspecific-finetuning-segment-anything.html</guid>
      <description>Promtless Task-Specific Finetuning of MetaAI Segment-Anything#Date: January 01, 2024#NOTE:#The NB was originally developed on Kaggle: https://www.kaggle.com/code/yogendrayatnalkar/promtless-taskspecific-finetuning-segment-anythingRelated Github Repository: https://github.com/yogendra-yatnalkar/SAM-Promptless-Task-Specific-Finetuning/tree/mainTask#Finetune SAM model on Custom dataset to segment objects without prompts (during training and inference)#Approach#How does SAM work (high-level):#Sam Encoder &amp;ndash;&amp;gt; ViT + Neck-Module (Consisting of 2 Conv2D layers used for downsampling the channels of the ViT output) The Encoder ViT has a patch-size of 16x16.</description>
    </item>
    
    <item>
      <title>End-to-End MLOps on AWS (3 blogs)</title>
      <link>https://yogendra-yatnalkar.github.io/blogs/end-to-end-mlops-on-aws.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/blogs/end-to-end-mlops-on-aws.html</guid>
      <description>End-to-End MLOps on AWS: Blog Series#Last Edit Date: December 26, 2023
I had co-authored 3 blogs out of 7 blogs in total on the topic: &amp;ldquo;End-to-End MLOps on AWS&amp;rdquo;. The blog series is hosted on Quantiphi&amp;rsquo;s Medium Account Other than Quantiphi&amp;rsquo;s Medium account, the blogs are also hosted seperately on github as follows:
https://sagemaker-mlops-samples.github.io/#Authors of the below 3 blogs:
Palash Nimodia(Architect â€” Machine Learning)</description>
    </item>
    
    <item>
      <title>Meta-AI SAM: AutoMatic Semantic Segmentation</title>
      <link>https://yogendra-yatnalkar.github.io/blogs/sam-automatic-semantic-segmentation.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/blogs/sam-automatic-semantic-segmentation.html</guid>
      <description>Meta-AI SAM: AutoMatic Semantic Segmentation#Date: November 26, 2023#NOTE:#The NB was originally developed on Kaggle: https://www.kaggle.com/code/yogendrayatnalkar/finetuning-segment-anythingRelated Github Repository: https://github.com/yogendra-yatnalkar/SAM-Automatic-Semantic-SegmentationTask:#Semantically segment objects from image AUTOMATICALLY with the help of META AI SAM, without PROMPTS/TRAINING#Segment all the pepperoni pieces from the pizza topping#Approach:#1. Automatic Mask Generation (AMG)
Utilizing the Segment Anything Model (SAM) from the MetaAI SAM repository, perform instance segmentation on the entire image.</description>
    </item>
    
    <item>
      <title>Backtracking AWS Lookout for Vision Service</title>
      <link>https://yogendra-yatnalkar.github.io/blogs/backtracking_aws_lookout_for_vision_service.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/blogs/backtracking_aws_lookout_for_vision_service.html</guid>
      <description>Backtracking AWS Lookout For Vision Service#The article tries to trace back AWS Lookout for Vision: Edge service model and successfully custom loads the model for inference (Just imagine the reduced inference cost ðŸ”¥)
Co-Author:Â Palash NimodiaDate: June 23, 2022
Medium Link: https://medium.com/@yogenyat/backtracking-aws-lookout-for-vision-service-136c47c85168Introduction:#NOTE for the reader: Its fine if you have not used AWS Lookout For Vision service before, but if you are interested in knowing how we can back-track a managed service (if possible ðŸ™ˆ), you are at the right place.</description>
    </item>
    
    <item>
      <title>Finding the nâ€™th Aggregate Value from Every Group in AWS Athena/Presto</title>
      <link>https://yogendra-yatnalkar.github.io/blogs/finding-nth-aggregate-from-every-group-aws-athena.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/blogs/finding-nth-aggregate-from-every-group-aws-athena.html</guid>
      <description>Finding the nâ€™th Aggregate Value from Every Group in AWS Athena/Presto#Co-Author:Â Palash NimodiaDate: June 2, 2022
Medium Link: https://medium.com/selectfrom/finding-the-nth-aggregate-value-from-every-group-in-aws-athena-presto-1da505310901Prerequisite:#Before going ahead, a quick read for those who donâ€™t know what AWS Athena and Presto are.
AWS Athena: Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is easy to use, fast, scalable and serverless, so there is no infrastructure to manage, and we pay only for the queries that we run.</description>
    </item>
    
    <item>
      <title>Benchmarking Inference with Torchserve</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/benchmarking-with-torchserve.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/benchmarking-with-torchserve.html</guid>
      <description>Benchmarking Inference with Torchserve#Last Edited 24/12/2023 Pytorch default - g4dn.xlarge#Notes:#Instance Type: ml.g4dn.xlarge
GPU: Nvidia T4 vCPU no: 4 CPU memory: 16 GB GPU memory: 16 GB Max RPS achieved: 32
With various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 32, the max RPS possible was only 32. Locust Configuration: Max Users: 200, Spawn Rate: 10 Max Response time at 95th percentile: ~5-6 sec</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://yogendra-yatnalkar.github.io/notes/nlp/bert.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/nlp/bert.html</guid>
      <description>BERT (Bidirectional Encoder Representation From Transformer)#Source:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - YouTubeOriginal Paper: https://arxiv.org/pdf/1810.04805v2.pdfSource:
BERT Neural Network - EXPLAINED! - YouTubeBefore BERT:#LSTM&amp;rsquo;s were used.
Problems:
Slow as each word is processed at a time (sequentially) Not truly bi-directional (left to right and right to left at a time in bidirectional LSTM) Bert Architecture: Multiple encoders stacked on each-other</description>
    </item>
    
    <item>
      <title>Daily-Scribble-2024</title>
      <link>https://yogendra-yatnalkar.github.io/notes/daily-scribble.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/daily-scribble.html</guid>
      <description>Daily Scribble:#Scribbling the content seen in the day:
January:#vLLM: Easy, fast, and cheap LLM serving for everyone
https://github.com/vllm-project/vllmInvented the concept of &amp;ldquo;PagedAttention&amp;rdquo; uses xFormer internally, so indirectly uses FlashAttention and FlashAttention2 as well Has support for continuous batching (very useful for transformer/decoder architecture) When released, claims to be much much faster than huggingface TGI. Now, even HF uses paged-attention for inference Fooocus:
Fooocus is an image generating software.</description>
    </item>
    
    <item>
      <title>General Notes</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/general-general.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/general-general.html</guid>
      <description>Linux:#Wget vs Curl#curl and wget both support http and various types of FTP protocols. curl is library but wget is a CLI tool. curl is used for both way data transfer (src to detination and vice-versa) But, wget can be used only for single way data transfer, example: downloading something form a web-server. </description>
    </item>
    
    <item>
      <title>Kullback-Leibler Divergence (KL Divergence)</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/kl-divergence.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/kl-divergence.html</guid>
      <description>Kullback-Leibler Divergence (KL Divergence)#Last Edited 25/06/2023 Definition:#Measures the distance between 2 prabability distributions Explanation + Proof:#Base Video: Intuitively Understanding the KL Divergence - YouTubeSequence of flips: H -&amp;gt; H -&amp;gt; T &amp;hellip;..
Multiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:
for True coin: P1 raise to something and P2 raise to something else</description>
    </item>
    
    <item>
      <title>Model Serving</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/model-serving.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/model-serving.html</guid>
      <description>Notes on Model Serving#Example: Torchserve, Tf-Serving, Triton, Flask, etc
TorchServe: - to check model status, I am using port 8081 - for inference, I am using port 8080 - if not using ts-config while deploying a model, it generates error - Question: - when to use 8080 vs 8081 &amp;ndash;&amp;gt; Inference api is bind to 8080, management api is bind to 8081 - how to load test ?</description>
    </item>
    
    <item>
      <title>NLP-General</title>
      <link>https://yogendra-yatnalkar.github.io/notes/nlp/nlp_general.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/nlp/nlp_general.html</guid>
      <description>NLP General:#I will keep on appending stuff which I read about NLP as and when I get time in this place This is mainly intended for two things: Quick glance on what I had read in past for a given topic If needed to deep-dive, just look at the sources I used while reading it for the first time Hackers Guide to Language Model:#Source: A Hackers&#39; Guide to Language Models - YouTubeNotebook: GitHub - fastai/lm-hackers: Hackers&#39; Guide to Language ModelsDate: 02/11/2023</description>
    </item>
    
    <item>
      <title>SAM-Segment-Anything</title>
      <link>https://yogendra-yatnalkar.github.io/notes/cv/sam-segment-anything.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/cv/sam-segment-anything.html</guid>
      <description>Segment Anything (SAM)#Last Edited 16/07/2023 Source: - Original Paper (notes till page no 7) - Youtube:Â https://www.youtube.com/watch?v=eYhvJR4zFUM Introduction:#Foundation model for segmentation.
Important thing to note here, is that SAM is not just for semantic segmentation but can also be used for instance or panoptic or salient segmentation as well. We just need to engineer it accordingly.
Trained ON: 11 million images and 1 billion corresponding masks</description>
    </item>
    
    <item>
      <title>SegFormer: Segmentation using Transformer</title>
      <link>https://yogendra-yatnalkar.github.io/notes/cv/segformer.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/cv/segformer.html</guid>
      <description>SegFormer: Segmentation using Transformer#Last Edited 16/07/2023 Source: SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers - YouTubeInput patch size: 4x4
In ViT, it was 16x16, but with smaller path size, the authors said, smaller batch size is better (and required) for dense prediction.
Note: With reducing the patch size, the computation increases.
After each transformer block (encoder in this case), there is a feed-forward block which is mainly used to lower the dimension like older UNet &amp;ndash;&amp;gt; i.</description>
    </item>
    
    <item>
      <title>Self Supervised Learning</title>
      <link>https://yogendra-yatnalkar.github.io/notes/cv/self-supervised-learning.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/cv/self-supervised-learning.html</guid>
      <description>A Cookbook of Self-Supervised Learning:#Initial Notes from: https://arxiv.org/abs/2304.12210 Intro:#NLP advanced due to SSL &amp;ndash;&amp;gt; No need of labelled data to train supervised model
SSL -&amp;gt; Define a pretext task &amp;ndash;&amp;gt; Un-labelled data &amp;ndash;&amp;gt; intelligent representation
NLP: Word2Vec is SSL &amp;ndash; In a sentence, mask a word and predict the surrounding words (It learns context)
CV: 2 current popular ways:
mask a patch and prediction of masked path</description>
    </item>
    
    <item>
      <title>Transformers at Training vs Inference</title>
      <link>https://yogendra-yatnalkar.github.io/notes/nlp/transformers_at_training_vs_inference.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/nlp/transformers_at_training_vs_inference.html</guid>
      <description>Transformers at Training vs Inference#</description>
    </item>
    
    <item>
      <title>Un-Answered Questions</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/unanswered-questions.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/unanswered-questions.html</guid>
      <description>Un-Answered Questions:#Difference between Float16 vs Bfloat16 vs Tensor-Float16 ?
Vector Databases: HNSW vs IVF ?
Difference between vector DB&amp;rsquo;s and FAISS library (by Meta) ?
From my current knowledge both are same, but then why is everyone behind vector DB&amp;rsquo;s instead of using FAISS directly ? Null Hypothesis test &amp;raquo; p-values &amp;raquo; calculated using t-test or z-test
Weeb Union Umar Jamil Cohehre&amp;rsquo;s embedding v3
RetNet &amp;ndash;&amp;gt; Saw it on high note &amp;ndash;&amp;gt; Removes the softmax from the Transformer network and adds a exponential-moving-average-weights to the network by which it gives more importance to the recent tokens</description>
    </item>
    
    <item>
      <title>Vector Search and Stores</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/vector-store-and-search.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/vector-store-and-search.html</guid>
      <description>Note: Just putting down few notes from AWS partner-cast session
Vector Search and Vector Stores#How to measure similarity in embeddings?#Cosine Similarity: Gives the angle between the 2 embeddings. Higher the angle, bigger is the difference between 2 embeddings.
Dot-Product: Same as cosine similarity but gives us the magnitude between 2 vectors instead of direction/angle.
Real-life Use cases:#Semantic search
Recommendation System
Anomaly detection and pattern recognition</description>
    </item>
    
    <item>
      <title>Web-API performance improvement</title>
      <link>https://yogendra-yatnalkar.github.io/notes/general/api-performance-improvement.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yogendra-yatnalkar.github.io/notes/general/api-performance-improvement.html</guid>
      <description>API Performance Improvement#Based on: Top 7 Ways to 10x Your API Performance - YouTubeOptimization should not be the first step of development
1. Caching:#If same request is repeated multiple times &amp;ndash;&amp;gt; cache hence no need to recompute or hit the DB again.
For DB, its: MemCacheD or Redis
2. Connection Pooling:#Having continues connections with DB can slow down server as each connection requires a lot of handshake protocol.</description>
    </item>
    
  </channel>
</rss>
