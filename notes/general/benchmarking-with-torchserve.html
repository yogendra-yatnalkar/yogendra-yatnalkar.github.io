<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Benchmarking Inference with Torchserve#Last Edited 05/01/2023 Pytorch default - g4dn.xlarge#Notes:#Instance Type: ml.g4dn.xlarge
GPU: Nvidia T4 vCPU no: 4 CPU memory: 16 GB GPU memory: 16 GB Max RPS achieved: 32
With various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 32, the max RPS possible was only 32. Locust Configuration: Max Users: 200, Spawn Rate: 10 Max Response time at 95th percentile: ~5-6 sec">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="Benchmarking Inference with Torchserve" />
<meta property="og:description" content="Benchmarking Inference with Torchserve#Last Edited 05/01/2023 Pytorch default - g4dn.xlarge#Notes:#Instance Type: ml.g4dn.xlarge
GPU: Nvidia T4 vCPU no: 4 CPU memory: 16 GB GPU memory: 16 GB Max RPS achieved: 32
With various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 32, the max RPS possible was only 32. Locust Configuration: Max Users: 200, Spawn Rate: 10 Max Response time at 95th percentile: ~5-6 sec" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yogendra-yatnalkar.github.io/notes/general/benchmarking-with-torchserve.html" /><meta property="article:section" content="notes" />


<title>Benchmarking Inference with Torchserve | Yogendra Y.</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.0933d5ebbed8c08a206e6d3b23fee14140516cfe9ea419244eaae5e9a0f2173e.css" integrity="sha256-CTPV677YwIogbm07I/7hQUBRbP6epBkkTqrl6aDyFz4=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.62b4147ae1119fb62dbffde5e4c97f9872450b658ed29f0f8aa232d56f812e92.js" integrity="sha256-YrQUeuERn7Ytv/3l5Ml/mHJFC2WO0p8PiqIy1W&#43;BLpI=" crossorigin="anonymous"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-DTLBL509Z2"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-DTLBL509Z2', { 'anonymize_ip': false });
}
</script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/logo.png" alt="Logo" /><span>Yogendra Y.</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  

  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c14ab2d8aecfbedfb55fbca3bdb6a6b" class="toggle"  />
    <label for="section-8c14ab2d8aecfbedfb55fbca3bdb6a6b" class="flex justify-between">
      <a href="/blogs.html" class="">Blogs</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/blogs/promtless-taskspecific-finetuning-segment-anything.html" class="">Promtless Task-Specific Finetuning of MetaAI Segment-Anything</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/blogs/end-to-end-mlops-on-aws.html" class="">End-to-End MLOps on AWS (3 blogs)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/blogs/sam-automatic-semantic-segmentation.html" class="">Meta-AI SAM: AutoMatic Semantic Segmentation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/blogs/backtracking_aws_lookout_for_vision_service.html" class="">Backtracking AWS Lookout for Vision Service</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/blogs/finding-nth-aggregate-from-every-group-aws-athena.html" class="">Finding the nâ€™th Aggregate Value from Every Group in AWS Athena/Presto</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ef46fe0aac274900fefdd5e564e236b2" class="toggle" checked />
    <label for="section-ef46fe0aac274900fefdd5e564e236b2" class="flex justify-between">
      <a role="button" class="">Notes</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/daily-scribble.html" class="">Daily-Scribble-2024</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-97f68e7cac7678f1405673189f8b189f" class="toggle" checked />
    <label for="section-97f68e7cac7678f1405673189f8b189f" class="flex justify-between">
      <a role="button" class="">General</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/general/benchmarking-with-torchserve.html" class="active">Benchmarking Inference with Torchserve</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/general-general.html" class="">General Notes</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/kl-divergence.html" class="">Kullback-Leibler Divergence (KL Divergence)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/model-serving.html" class="">Model Serving</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/unanswered-questions.html" class="">Un-Answered Questions</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/vector-store-and-search.html" class="">Vector Search and Stores</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/general/api-performance-improvement.html" class="">Web-API performance improvement</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8314eced0a8c88614b1d2f53940330d5" class="toggle"  />
    <label for="section-8314eced0a8c88614b1d2f53940330d5" class="flex justify-between">
      <a role="button" class="">CV</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/cv/sam-segment-anything.html" class="">SAM-Segment-Anything</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/cv/segformer.html" class="">SegFormer: Segmentation using Transformer</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/cv/self-supervised-learning.html" class="">Self Supervised Learning</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-5bf3042abef6b7e58a1750f2ab25ff7e" class="toggle"  />
    <label for="section-5bf3042abef6b7e58a1750f2ab25ff7e" class="flex justify-between">
      <a role="button" class="">NLP</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/nlp/bert.html" class="">BERT</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/nlp/nlp_general.html" class="">NLP-General</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/nlp/transformers_at_training_vs_inference.html" class="">Transformers at Training vs Inference</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/resume/" class="">Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-77dedc5429a7c70061bd8ace32d09bfc" class="toggle"  />
    <label for="section-77dedc5429a7c70061bd8ace32d09bfc" class="flex justify-between">
      <a href="/side-projects.html" class="">Side-Projects</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/side-projects/ai-assisted-video-generation.html" class="">AI Assisted Video Generation with White-Board Animations</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Benchmarking Inference with Torchserve</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#pytorch-default---g4dnxlarge">Pytorch default - g4dn.xlarge</a>
      <ul>
        <li><a href="#notes">Notes:</a></li>
        <li><a href="#configuration">Configuration:</a></li>
      </ul>
    </li>
    <li><a href="#pytorch-default---g4dn2xlarge">Pytorch default - g4dn.2xlarge</a>
      <ul>
        <li><a href="#notes-1">Notes:</a></li>
        <li><a href="#configuration-1">Configuration:</a></li>
      </ul>
    </li>
    <li><a href="#does-dynamic-batching-really-help--ps-it-dooes">Does Dynamic Batching Really Help  (PS: It dooes)</a>
      <ul>
        <li><a href="#batch-size-1-and-workers-1">Batch-size 1 and workers 1:</a></li>
        <li><a href="#configuration-2">Configuration:</a></li>
        <li><a href="#batch-size-1-and-workers-4">Batch-size 1 and workers 4:</a></li>
        <li><a href="#configuration-3">Configuration:</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="benchmarking-inference-with-torchserve">
  Benchmarking Inference with Torchserve
  <a class="anchor" href="#benchmarking-inference-with-torchserve">#</a>
</h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Last Edited</td>
<td>05/01/2023</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="pytorch-default---g4dnxlarge">
  Pytorch default - g4dn.xlarge
  <a class="anchor" href="#pytorch-default---g4dnxlarge">#</a>
</h2>
<h3 id="notes">
  Notes:
  <a class="anchor" href="#notes">#</a>
</h3>
<ul>
<li>
<p><strong>Instance Type: ml.g4dn.xlarge</strong></p>
<ul>
<li>GPU: Nvidia T4</li>
<li>vCPU no: 4</li>
<li>CPU memory: 16 GB</li>
<li>GPU memory: 16 GB</li>
</ul>
</li>
<li>
<p><strong>Max RPS achieved: 32</strong></p>
<ul>
<li>With various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 32, the max RPS possible was only 32.
<ul>
<li>Locust Configuration: Max Users: 200, Spawn Rate: 10</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Max Response time at 95th percentile: ~5-6 sec</strong></p>
</li>
</ul>
<hr>
<p><img src="benchmarking-with-torchserve/image-3.png" alt="Alt text" /></p>
<p><img src="benchmarking-with-torchserve/image-1.png" alt="Alt text" /></p>
<h3 id="configuration">
  Configuration:
  <a class="anchor" href="#configuration">#</a>
</h3>
<pre tabindex="0"><code>enable_envvars_config=true
load_models=all
model_store=./model_store
models={\
  &#34;vit_l_16&#34;: {\
    &#34;1.0&#34;: {\
        &#34;defaultVersion&#34;: true,\
        &#34;marName&#34;: &#34;vit_l_16.mar&#34;,\
        &#34;minWorkers&#34;: 4,\
        &#34;maxWorkers&#34;: 4,\
        &#34;batchSize&#34;: 16,\
        &#34;maxBatchDelay&#34;: 50\
    }\
  }\
}
</code></pre><hr>
<hr>
<h2 id="pytorch-default---g4dn2xlarge">
  Pytorch default - g4dn.2xlarge
  <a class="anchor" href="#pytorch-default---g4dn2xlarge">#</a>
</h2>
<h3 id="notes-1">
  Notes:
  <a class="anchor" href="#notes-1">#</a>
</h3>
<ul>
<li>
<p><strong>Instance Type: ml.g4dn.2xlarge</strong></p>
<ul>
<li>GPU: Nvidia T4</li>
<li>vCPU no: 8</li>
<li>CPU memory: 32 GB</li>
<li>GPU memory: 16 GB</li>
</ul>
</li>
<li>
<p><strong>Max RPS achieved: 32</strong></p>
<ul>
<li>With various different configuration ranging from min/max worker = 1 to 4 and batch-size 4 to 64, the max RPS possible was only around 32.
<ul>
<li>Locust Configuration: Max Users: 200, Spawn Rate: 10</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Max Response time at 95th percentile: ~5-6 sec</strong></p>
</li>
</ul>
<hr>
<p><img src="benchmarking-with-torchserve/image.png" alt="Alt text" /></p>
<ul>
<li>It can be noted that the GPU utilization is at 100% but the gpu memory is underutilized and the vCPU&rsquo;s are also unutilized</li>
<li>Never the less, with any change in configuration in number-of-model-workers or batch-size or delay, the results and utilization numbers does not change.</li>
</ul>
<p><img src="benchmarking-with-torchserve/image-2.png" alt="Alt text" /></p>
<h3 id="configuration-1">
  Configuration:
  <a class="anchor" href="#configuration-1">#</a>
</h3>
<pre tabindex="0"><code>enable_envvars_config=true
load_models=all
model_store=./model_store
models={\
  &#34;vit_l_16&#34;: {\
    &#34;1.0&#34;: {\
        &#34;defaultVersion&#34;: true,\
        &#34;marName&#34;: &#34;vit_l_16.mar&#34;,\
        &#34;minWorkers&#34;: 1,\
        &#34;maxWorkers&#34;: 1,\
        &#34;batchSize&#34;: 64,\
        &#34;maxBatchDelay&#34;: 200,\
        &#34;responseTimeout&#34;: 240\
    }\
  }\
}
</code></pre><hr>
<hr>
<h2 id="does-dynamic-batching-really-help--ps-it-dooes">
  Does Dynamic Batching Really Help  (PS: It dooes)
  <a class="anchor" href="#does-dynamic-batching-really-help--ps-it-dooes">#</a>
</h2>
<p>Instance used: (G4dn.2xlarge)</p>
<ul>
<li>I can imagine 2 scenarioes,
<ol>
<li>where workers is set to 1 and batch size 1</li>
<li>where workers &gt; 1 (atleast 4) and batch-size 1</li>
</ol>
</li>
</ul>
<h3 id="batch-size-1-and-workers-1">
  Batch-size 1 and workers 1:
  <a class="anchor" href="#batch-size-1-and-workers-1">#</a>
</h3>
<ul>
<li>
<p><strong>Instance Type: ml.g4dn.2xlarge</strong></p>
<ul>
<li>GPU: Nvidia T4</li>
<li>vCPU no: 8</li>
<li>CPU memory: 32 GB</li>
<li>GPU memory: 16 GB</li>
</ul>
</li>
<li>
<p><strong>Max RPS achieved: 21</strong></p>
<ul>
<li>min/max workers = 1 and batch-size = 1, the max RPS possible was only around 21.
<ul>
<li>Locust Configuration: Max Users: 200, Spawn Rate: 10</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Max Response time at 95th percentile: ~5 sec (close to 4.9 sec)</strong></p>
<ul>
<li>Its a bit less than dynamic batching as torchserve does not have to wait for extra time for creation of batches</li>
</ul>
</li>
</ul>
<hr>
<p><img src="benchmarking-with-torchserve/image-4.png" alt="Alt text" /></p>
<ul>
<li><strong>Note: The GPU utilization is also not full</strong></li>
</ul>
<p><img src="benchmarking-with-torchserve/image-7.png" alt="Alt text" /></p>
<h3 id="configuration-2">
  Configuration:
  <a class="anchor" href="#configuration-2">#</a>
</h3>
<pre tabindex="0"><code>enable_envvars_config=true
load_models=all
model_store=./model_store
models={\
  &#34;vit_l_16&#34;: {\
    &#34;1.0&#34;: {\
        &#34;defaultVersion&#34;: true,\
        &#34;marName&#34;: &#34;vit_l_16.mar&#34;,\
        &#34;minWorkers&#34;: 1,\
        &#34;maxWorkers&#34;: 1,\
        &#34;batchSize&#34;: 1,\
        &#34;maxBatchDelay&#34;: 200,\
        &#34;responseTimeout&#34;: 240\
    }\
  }\
}
</code></pre><hr>
<hr>
<h3 id="batch-size-1-and-workers-4">
  Batch-size 1 and workers 4:
  <a class="anchor" href="#batch-size-1-and-workers-4">#</a>
</h3>
<ul>
<li>
<p>Note: For some reason, with workers 4, the GPU utilization is 100 but the RPS is still the same</p>
</li>
<li>
<p><strong>Instance Type: ml.g4dn.2xlarge</strong></p>
<ul>
<li>GPU: Nvidia T4</li>
<li>vCPU no: 8</li>
<li>CPU memory: 32 GB</li>
<li>GPU memory: 16 GB</li>
</ul>
</li>
<li>
<p><strong>Max RPS achieved: 21</strong></p>
<ul>
<li>min/max workers = 4 and batch-size = 1, the max RPS possible was only around 21.
<ul>
<li>Locust Configuration: Max Users: 200, Spawn Rate: 10</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Max Response time at 95th percentile: ~5 sec (close to 4.9 sec)</strong></p>
<ul>
<li>Its a bit less than dynamic batching as torchserve does not have to wait for extra time for creation of batches</li>
</ul>
</li>
</ul>
<hr>
<p><img src="benchmarking-with-torchserve/image-6.png" alt="Alt text" /></p>
<p><img src="benchmarking-with-torchserve/image-5.png" alt="Alt text" /></p>
<h3 id="configuration-3">
  Configuration:
  <a class="anchor" href="#configuration-3">#</a>
</h3>
<pre tabindex="0"><code>enable_envvars_config=true
load_models=all
model_store=./model_store
models={\
  &#34;vit_l_16&#34;: {\
    &#34;1.0&#34;: {\
        &#34;defaultVersion&#34;: true,\
        &#34;marName&#34;: &#34;vit_l_16.mar&#34;,\
        &#34;minWorkers&#34;: 1,\
        &#34;maxWorkers&#34;: 1,\
        &#34;batchSize&#34;: 1,\
        &#34;maxBatchDelay&#34;: 200,\
        &#34;responseTimeout&#34;: 240\
    }\
  }\
}
</code></pre></article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments"><script src="https://utteranc.es/client.js"
        repo="yogendra-yatnalkar/yogendra-yatnalkar.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>
</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#pytorch-default---g4dnxlarge">Pytorch default - g4dn.xlarge</a>
      <ul>
        <li><a href="#notes">Notes:</a></li>
        <li><a href="#configuration">Configuration:</a></li>
      </ul>
    </li>
    <li><a href="#pytorch-default---g4dn2xlarge">Pytorch default - g4dn.2xlarge</a>
      <ul>
        <li><a href="#notes-1">Notes:</a></li>
        <li><a href="#configuration-1">Configuration:</a></li>
      </ul>
    </li>
    <li><a href="#does-dynamic-batching-really-help--ps-it-dooes">Does Dynamic Batching Really Help  (PS: It dooes)</a>
      <ul>
        <li><a href="#batch-size-1-and-workers-1">Batch-size 1 and workers 1:</a></li>
        <li><a href="#configuration-2">Configuration:</a></li>
        <li><a href="#batch-size-1-and-workers-4">Batch-size 1 and workers 4:</a></li>
        <li><a href="#configuration-3">Configuration:</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












