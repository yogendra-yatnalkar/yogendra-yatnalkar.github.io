<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Kullback-Leibler Divergence (KL Divergence)#Definition:#Measures the distance between 2 prabability distributions Explanation &#43; Proof:#Base Video: Intuitively Understanding the KL Divergence - YouTubeSequence of flips: H -&gt; H -&gt; T &hellip;..
Multiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:
for True coin: P1 raise to something and P2 raise to something else
For coin2: Q1 raise to soemthing and Q2 raise to something else">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="Kullback-Leibler Divergence (KL Divergence)" />
<meta property="og:description" content="Kullback-Leibler Divergence (KL Divergence)#Definition:#Measures the distance between 2 prabability distributions Explanation &#43; Proof:#Base Video: Intuitively Understanding the KL Divergence - YouTubeSequence of flips: H -&gt; H -&gt; T &hellip;..
Multiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:
for True coin: P1 raise to something and P2 raise to something else
For coin2: Q1 raise to soemthing and Q2 raise to something else" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yogendra-yatnalkar.github.io/notes/kl-divergence.html" /><meta property="article:section" content="notes" />


<title>Kullback-Leibler Divergence (KL Divergence) | Activated Neuron</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.0933d5ebbed8c08a206e6d3b23fee14140516cfe9ea419244eaae5e9a0f2173e.css" integrity="sha256-CTPV677YwIogbm07I/7hQUBRbP6epBkkTqrl6aDyFz4=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.f65589be9a18e69eb98489c41e8e336db79d4c41c9fa1d7014c664a136677587.js" integrity="sha256-9lWJvpoY5p65hInEHo4zbbedTEHJ&#43;h1wFMZkoTZndYc=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><img src="/logo.png" alt="Logo" /><span>Activated Neuron</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  

  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ef46fe0aac274900fefdd5e564e236b2" class="toggle" checked />
    <label for="section-ef46fe0aac274900fefdd5e564e236b2" class="flex justify-between">
      <a role="button" class="">Notes</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/kl-divergence.html" class="active">Kullback-Leibler Divergence (KL Divergence)</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8c14ab2d8aecfbedfb55fbca3bdb6a6b" class="toggle"  />
    <label for="section-8c14ab2d8aecfbedfb55fbca3bdb6a6b" class="flex justify-between">
      <a role="button" class="">Blogs</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>















</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Kullback-Leibler Divergence (KL Divergence)</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#definition">Definition:</a></li>
        <li><a href="#explanation--proof">Explanation + Proof:</a></li>
      </ul>
    </li>
    <li><a href="#-why-take-log-of-probability-">** Why take log of probability ?</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="kullback-leibler-divergence-kl-divergence">
  Kullback-Leibler Divergence (KL Divergence)
  <a class="anchor" href="#kullback-leibler-divergence-kl-divergence">#</a>
</h1>
<h3 id="definition">
  Definition:
  <a class="anchor" href="#definition">#</a>
</h3>
<ul>
<li>Measures the distance between 2 prabability distributions</li>
</ul>
<h3 id="explanation--proof">
  Explanation + Proof:
  <a class="anchor" href="#explanation--proof">#</a>
</h3>
<p>Base Video: <a href="https://www.youtube.com/watch?v=SxGYPqCgJWM" target="_blank" rel="noopener">Intuitively Understanding the KL Divergence - YouTube</a>
</p>
<p><img src="kl-divergence/2023-06-25-13-09-22-image.png" alt="" /></p>
<p>Sequence of flips: <strong>H -&gt; H -&gt; T &hellip;..</strong></p>
<p>Multiply the probabilities from both the coins for the corresponding heads and tails. It is nothing but:</p>
<ul>
<li>
<p>for True coin: P1 raise to something and P2 raise to something else</p>
</li>
<li>
<p>For coin2: Q1 raise to soemthing and Q2 raise to something else</p>
</li>
</ul>
<p><img src="kl-divergence/2023-06-25-13-13-33-image.png" alt="" /></p>
<ul>
<li>
<p>after applying log to the RHS: (** &ndash;&gt; Explained at the end)</p>
<p><img src="kl-divergence/2023-06-25-13-14-41-image.png" alt="" /></p>
</li>
<li>
<p>As the number of observations tends towards infinity:</p>
<ul>
<li>
<p><strong>Nh/n ~~ p1</strong></p>
</li>
<li>
<p><strong>Nt/N ~~ p2</strong></p>
</li>
</ul>
<p>This leads us to the final log expression:</p>
</li>
</ul>
<p><img src="kl-divergence/2023-06-25-13-23-53-image.png" alt="" /></p>
<h4 id="general-fomulae">
  General Fomulae:
  <a class="anchor" href="#general-fomulae">#</a>
</h4>
<p><img src="kl-divergence/37beef4003f8bc42829a3442f26431d7c02b70a4.png" alt="" /></p>
<p>&ldquo;This computes the distance between 2 distributions motivated by looking at how likely the 2nd distribution would be able to generate samples from the first distribution&rdquo;</p>
<p><strong>Cross-entropy Loss is very related to KL Divergence</strong></p>
<hr>
<h2 id="-why-take-log-of-probability-">
  ** Why take log of probability ?
  <a class="anchor" href="#-why-take-log-of-probability-">#</a>
</h2>
<p><em><strong>From the probabilities of ratio, why did we suddenly take log of ratio ??</strong></em></p>
<ul>
<li>
<p>The log of probabilities is closely related entropy. In <a href="https://en.wikipedia.org/wiki/Information_theory" title="Information theory" target="_blank" rel="noopener">information theory</a>
, the <strong>entropy</strong> of a <a href="https://en.wikipedia.org/wiki/Random_variable" title="Random variable" target="_blank" rel="noopener">random variable</a>
 is the average level of &ldquo;information&rdquo;, &ldquo;surprise&rdquo;, or &ldquo;uncertainty&rdquo; inherent to the variable&rsquo;s possible outcomes.</p>
<p><img src="kl-divergence/2023-06-25-16-12-19-image.png" alt="" /></p>
<h5 id="kl-divergence-is-also-known-as-relative-entropy-between-2-distributions">
  KL Divergence is also known as relative entropy between 2 distributions.
  <a class="anchor" href="#kl-divergence-is-also-known-as-relative-entropy-between-2-distributions">#</a>
</h5>
</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments"><script src="https://utteranc.es/client.js"
        repo="yogendra-yatnalkar/yogendra-yatnalkar.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>
</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#definition">Definition:</a></li>
        <li><a href="#explanation--proof">Explanation + Proof:</a></li>
      </ul>
    </li>
    <li><a href="#-why-take-log-of-probability-">** Why take log of probability ?</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












